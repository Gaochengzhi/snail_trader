import time
import logging
from typing import Optional, Dict, Any
from dataclasses import dataclass
import requests
from openai import OpenAI
from pathlib import Path
import sys
import socket
import threading
from queue import Queue, PriorityQueue
from concurrent.futures import ThreadPoolExecutor, Future
import uuid

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥config_utils
sys.path.append(str(Path(__file__).parent.parent))
from utils.config_utils import Config

logger = logging.getLogger(__name__)


@dataclass
class ProviderConfig:
    """å•ä¸ªLLMæä¾›å•†é…ç½®"""
    provider: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    model: str = ""


class ConcurrencyManager:
    """ç»Ÿä¸€å¹¶å‘æ§åˆ¶ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.concurrency_enabled = config.get("concurrency_control", {}).get("enabled", True)
        
        # ä¸ºæ¯ä¸ªprovideråˆ›å»ºä¿¡å·é‡å’Œçº¿ç¨‹æ± 
        self.provider_semaphores: Dict[str, threading.Semaphore] = {}
        self.provider_executors: Dict[str, ThreadPoolExecutor] = {}
        
        # å…¨å±€è¯·æ±‚é˜Ÿåˆ—
        self.global_queue_size = config.get("concurrency_control", {}).get("global_queue_size", 100)
        self.request_queue = PriorityQueue(maxsize=self.global_queue_size)
        self.queue_worker_running = False
        
        self._init_provider_controls()
    
    def _init_provider_controls(self):
        """ä¸ºæ¯ä¸ªprovideråˆå§‹åŒ–å¹¶å‘æ§åˆ¶"""
        providers = self.config.get("providers", {})
        for provider_name, provider_config in providers.items():
            max_concurrent = provider_config.get("max_concurrent", 3)
            
            # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
            self.provider_semaphores[provider_name] = threading.Semaphore(max_concurrent)
            
            # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œè¯·æ±‚
            self.provider_executors[provider_name] = ThreadPoolExecutor(
                max_workers=max_concurrent,
                thread_name_prefix=f"llm-{provider_name}"
            )
            
            logger.debug(f"Provider {provider_name} æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
    
    def execute_request(self, provider_name: str, request_func, *args, **kwargs):
        """æ‰§è¡Œå¸¦å¹¶å‘æ§åˆ¶çš„è¯·æ±‚"""
        if not self.concurrency_enabled:
            return request_func(*args, **kwargs)
        
        semaphore = self.provider_semaphores.get(provider_name)
        executor = self.provider_executors.get(provider_name)
        
        if not semaphore or not executor:
            logger.warning(f"Provider {provider_name} å¹¶å‘æ§åˆ¶æœªåˆå§‹åŒ–ï¼Œç›´æ¥æ‰§è¡Œ")
            return request_func(*args, **kwargs)
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        with semaphore:
            logger.debug(f"Provider {provider_name} è·å–å¹¶å‘è®¸å¯ï¼Œå½“å‰å¯ç”¨: {semaphore._value}")
            try:
                return request_func(*args, **kwargs)
            finally:
                logger.debug(f"Provider {provider_name} é‡Šæ”¾å¹¶å‘è®¸å¯")
    
    def get_provider_status(self, provider_name: str) -> Dict[str, Any]:
        """è·å–providerå¹¶å‘çŠ¶æ€"""
        semaphore = self.provider_semaphores.get(provider_name)
        executor = self.provider_executors.get(provider_name)
        provider_config = self.config.get("providers", {}).get(provider_name, {})
        
        if not semaphore or not executor:
            return {"status": "not_initialized"}
        
        max_concurrent = provider_config.get("max_concurrent", 3)
        current_concurrent = max_concurrent - semaphore._value
        
        return {
            "max_concurrent": max_concurrent,
            "current_concurrent": current_concurrent,
            "available_slots": semaphore._value,
            "queue_size": executor._work_queue.qsize() if hasattr(executor._work_queue, 'qsize') else 0
        }
    
    def shutdown(self):
        """å…³é—­æ‰€æœ‰çº¿ç¨‹æ± """
        for executor in self.provider_executors.values():
            executor.shutdown(wait=True)


class UniversalLLM:
    """ç»Ÿä¸€LLMæ¥å£ï¼Œæ”¯æŒå¤šä¸ªLLMæä¾›å•†çš„fallbackåˆ‡æ¢"""
    
    _instance = None
    _lock = threading.Lock()

    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(UniversalLLM, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if hasattr(self, '_initialized'):
            return
            
        self.config = self._load_config_from_yaml()
        self._clients: Dict[str, Any] = {}
        self.current_provider = None
        
        # è¶…æ—¶é…ç½®
        self.connection_timeout = self.config.get("connection_timeout", 10)
        self.first_byte_timeout = self.config.get("first_byte_timeout", 30) 
        self.total_timeout = self.config.get("total_timeout", 120)
        
        # åˆå§‹åŒ–å¹¶å‘æ§åˆ¶ç®¡ç†å™¨
        self.concurrency_manager = ConcurrencyManager(self.config)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_clients()
        self._initialized = True

    def _load_config_from_yaml(self) -> Dict[str, Any]:
        """ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            config_loader = Config()
            yaml_config = config_loader.settings.model_dump()
            return yaml_config.get("llm", {})
        except Exception as e:
            logger.error(f"åŠ è½½YAMLé…ç½®å¤±è´¥: {e}")
            raise



    def _init_clients(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        providers = self.config.get("providers", {})
        for provider_name, provider_config in providers.items():
            try:
                provider_type = provider_config.get("provider")
                if provider_type in ["openai", "gemini", "openroute", "gemini_flash"]:
                    api_key = provider_config.get("api_key")
                    base_url = provider_config.get("base_url")
                    if api_key:
                        self._clients[provider_name] = OpenAI(
                            api_key=api_key,
                            base_url=base_url,
                            timeout=self.total_timeout
                        )
                        logger.debug(f"å®¢æˆ·ç«¯ '{provider_name}' åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"åˆå§‹åŒ–å®¢æˆ·ç«¯ '{provider_name}' å¤±è´¥: {e}")

    def _call_openai_compatible(self, provider_name: str, prompt: str) -> str:
        """è°ƒç”¨OpenAIå…¼å®¹çš„API"""
        def _execute_request():
            client = self._clients.get(provider_name)
            if not client:
                raise ValueError(f"å®¢æˆ·ç«¯ '{provider_name}' æœªåˆå§‹åŒ–")

            provider_config = self.config["providers"][provider_name]
            messages = [{"role": "user", "content": prompt}]

            kwargs = {
                "model": provider_config["model"],
                "messages": messages,
                "temperature": self.config.get("temperature", 0.7),
            }

            max_tokens = self.config.get("max_tokens")
            if max_tokens:
                kwargs["max_tokens"] = max_tokens

            response = client.chat.completions.create(**kwargs)
            return response.choices[0].message.content
        
        return self.concurrency_manager.execute_request(provider_name, _execute_request)

    def _call_ollama(self, provider_name: str, prompt: str) -> str:
        """è°ƒç”¨Ollama API"""
        def _execute_request():
            provider_config = self.config["providers"][provider_name]
            url = f"{provider_config['base_url']}/api/generate"

            payload = {
                "model": provider_config["model"],
                "prompt": prompt,
                "stream": False,
                "options": {"temperature": self.config.get("temperature", 0.7)},
            }

            max_tokens = self.config.get("max_tokens")
            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            # ä½¿ç”¨åˆ†å±‚è¶…æ—¶ï¼š(è¿æ¥è¶…æ—¶, è¯»å–è¶…æ—¶)
            timeout = (self.connection_timeout, self.first_byte_timeout)
            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()
            result = response.json()
            return result.get("response", "")
        
        return self.concurrency_manager.execute_request(provider_name, _execute_request)


    def generate(self, prompt: str, verbose: bool = False) -> str:
        """
        ç”Ÿæˆå›å¤ï¼Œæ”¯æŒfallbackåˆ‡æ¢ï¼Œæ”¹è¿›çš„è¶…æ—¶å¤„ç†
        """
        active_fallback_list = self.config.get("active_fallback_list", [])
        max_retries = self.config.get("max_retries", 3)
        retry_delay = self.config.get("retry_delay", 1.0)
        
        for provider_name in active_fallback_list:
            if provider_name not in self.config.get("providers", {}):
                if verbose:
                    print(f"âš ï¸ Provider '{provider_name}' é…ç½®æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
                continue

            for attempt in range(max_retries):
                try:
                    if verbose:
                        print(f"ä½¿ç”¨ {provider_name} è°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})...")
                    
                    result = self._call_provider_with_timeout(provider_name, prompt)
                    
                    if verbose:
                        print(f"âœ… {provider_name} è°ƒç”¨æˆåŠŸ")
                    
                    self.current_provider = provider_name
                    return result

                except (requests.exceptions.Timeout, socket.timeout) as e:
                    if verbose:
                        print(f"â±ï¸ {provider_name} è¶…æ—¶: {e}")
                    # è¶…æ—¶é”™è¯¯ç›´æ¥è·³åˆ°ä¸‹ä¸€ä¸ªproviderï¼Œä¸æµªè´¹é‡è¯•æ¬¡æ•°
                    break
                    
                except Exception as e:
                    if verbose:
                        print(f"âŒ {provider_name} å°è¯• {attempt + 1} å¤±è´¥: {e}")
                    
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                    else:
                        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè·³åˆ°ä¸‹ä¸€ä¸ªprovider
                        if verbose:
                            print(f"ğŸ”„ {provider_name} æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªprovider")

        raise Exception("æ‰€æœ‰LLM fallbackéƒ½å¤±è´¥")

    def _call_provider_with_timeout(self, provider_name: str, prompt: str) -> str:
        """å¸¦è¶…æ—¶æ§åˆ¶çš„providerè°ƒç”¨"""
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError(f"Provider {provider_name} é¦–å­—èŠ‚è¶…æ—¶ ({self.first_byte_timeout}s)")
        
        try:
            # è®¾ç½®ä¿¡å·è¶…æ—¶ï¼ˆç”¨äºæ£€æµ‹é¦–å­—èŠ‚è¶…æ—¶ï¼‰
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(self.first_byte_timeout)
            
            result = self._call_provider(provider_name, prompt)
            
            # å–æ¶ˆè¶…æ—¶
            signal.alarm(0)
            return result
            
        except TimeoutError:
            signal.alarm(0)
            raise requests.exceptions.Timeout(f"é¦–å­—èŠ‚è¶…æ—¶: {self.first_byte_timeout}s")
        except Exception:
            signal.alarm(0)
            raise

    def _call_provider(self, provider_name: str, prompt: str) -> str:
        """è°ƒç”¨æŒ‡å®šçš„provider"""
        provider_config = self.config["providers"][provider_name]
        provider_type = provider_config.get("provider")

        if provider_type in ["openai", "gemini", "openroute", "gemini_flash"]:
            return self._call_openai_compatible(provider_name, prompt)
        elif provider_type == "ollama":
            return self._call_ollama(provider_name, prompt)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„providerç±»å‹: {provider_type}")

    def get_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€ä¿¡æ¯"""
        providers = self.config.get("providers", {})
        current_provider_config = providers.get(self.current_provider, {})
        
        # è·å–æ¯ä¸ªproviderçš„å¹¶å‘çŠ¶æ€
        provider_concurrency_status = {}
        for provider_name in providers.keys():
            provider_concurrency_status[provider_name] = self.concurrency_manager.get_provider_status(provider_name)
        
        return {
            "active_fallback_list": self.config.get("active_fallback_list", []),
            "current_provider": self.current_provider,
            "current_provider_type": current_provider_config.get("provider"),
            "current_model": current_provider_config.get("model"),
            "enable_streaming": self.config.get("enable_streaming", False),
            "max_retries": self.config.get("max_retries", 3),
            "temperature": self.config.get("temperature", 0.7),
            "connection_timeout": self.connection_timeout,
            "first_byte_timeout": self.first_byte_timeout,
            "total_timeout": self.total_timeout,
            "concurrency_enabled": self.concurrency_manager.concurrency_enabled,
            "provider_concurrency_status": provider_concurrency_status,
            "available_providers": list(providers.keys()),
            "initialized_clients": list(self._clients.keys()),
        }
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        if hasattr(self, 'concurrency_manager'):
            self.concurrency_manager.shutdown()


def create_llm() -> UniversalLLM:
    """åˆ›å»ºLLMå®ä¾‹"""
    return UniversalLLM()
