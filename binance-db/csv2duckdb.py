# csv2duckdb.py - Binance Data ETL Tool
#
# Copyright (c) 2025 Gaochengzhi
# Licensed under MIT License with Commercial Use Restriction
#
# This software is free for personal and research use.
# Commercial use by organizations requires explicit permission.
# See LICENSE file for details.
#
import duckdb, pathlib, os, re
from multiprocessing import Pool
import pyarrow.csv as pv
import pyarrow as pa
import pyarrow.parquet as pq
from datetime import datetime, timezone
from tqdm import tqdm
import logging
import yaml


# ============================================================================
# ğŸ“‹ é…ç½®æ–‡ä»¶åŠ è½½
# ============================================================================
def load_config():
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config


config = load_config()

# ä»config.yamlè¯»å–é…ç½®  
output_dir = pathlib.Path(config["output_directory"]).expanduser().absolute()  # å¤„ç†~å’Œç›¸å¯¹è·¯å¾„ï¼Œä½†ä¿ç•™è½¯é“¾æ¥
DATA_PATH = str(output_dir / "binance_parquet")  # Parquetè¾“å‡ºç›®å½•
SRC_DIR = output_dir  # CSVæºæ•°æ®ç›®å½•
DUCK_DB = str(output_dir / "binance.duckdb")  # DuckDBæ•°æ®åº“æ–‡ä»¶
LOG_DIR = config["log_directory"]  # æ—¥å¿—æ–‡ä»¶ç›®å½•
CPU = os.cpu_count()  # é»˜è®¤ä½¿ç”¨æœ€å¤§CPUæ ¸å¿ƒæ•°

# ============================================================================
# ğŸ§ª æµ‹è¯•é…ç½®åŒºåŸŸ - æ€§èƒ½æµ‹è¯•å‚æ•°
# ============================================================================
# çŸ­æ—¶é—´èŒƒå›´æµ‹è¯• (ç”¨äºå…¨ç›˜æ‰«æï¼Œæ•°æ®å¯†é›†å‹)
TEST_SHORT_START = datetime(2025, 8, 1, 0, 0, tzinfo=timezone.utc)
TEST_SHORT_END = datetime(2025, 8, 2, 4, 0, tzinfo=timezone.utc)  # 2å°æ—¶çª—å£

# é•¿æ—¶é—´èŒƒå›´æµ‹è¯• (ç”¨äºå•ç¬¦å·æ—¶åºåˆ†æï¼Œæ—¶é—´è·¨åº¦å¤§)
TEST_LONG_START = datetime(2024, 7, 29, 0, 0, tzinfo=timezone.utc)  # 3å¤©æ•°æ®
TEST_LONG_END = datetime(2025, 8, 1, 23, 59, tzinfo=timezone.utc)

# Convert to millisecond timestamps
TEST_SHORT_START_MS = int(TEST_SHORT_START.timestamp() * 1000)
TEST_SHORT_END_MS = int(TEST_SHORT_END.timestamp() * 1000)
TEST_LONG_START_MS = int(TEST_LONG_START.timestamp() * 1000)
TEST_LONG_END_MS = int(TEST_LONG_END.timestamp() * 1000)

# ä¿æŒåŸæœ‰å…¼å®¹æ€§
START_TIME = TEST_SHORT_START
END_TIME = TEST_SHORT_END
START_TIME_MS = TEST_SHORT_START_MS
END_TIME_MS = TEST_SHORT_END_MS
# ============================================================================
# ğŸ“ ç›®å½•åˆå§‹åŒ–å’Œæ—¥å¿—é…ç½®
# ============================================================================
DST_DIR = pathlib.Path(DATA_PATH)
LOG_PATH = pathlib.Path(LOG_DIR)

# ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
for directory in [DST_DIR, LOG_PATH]:
    directory.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
LOG_FILE = LOG_PATH / "conversion.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler(LOG_FILE, encoding="utf-8"), logging.StreamHandler()],
)

# å¯åŠ¨æ—¶è®°å½•é…ç½®ä¿¡æ¯
logging.info(f"=== ETLå·¥å…·å¯åŠ¨ ===")
logging.info(f"æºç›®å½•: {SRC_DIR}")
logging.info(f"ç›®æ ‡ç›®å½•: {DST_DIR}")
logging.info(f"æ—¥å¿—ç›®å½•: {LOG_PATH}")
logging.info(f"æ•°æ®åº“: {DUCK_DB}")

CSV_REGEX = re.compile(
    r"(?P<symbol>[A-Z0-9]+)-(?P<kind_or_interval>[a-zA-Z0-9]+)-(?P<date>\d{4}-\d{2}-\d{2})\.csv$"
)


# ---------- 1. å•æ–‡ä»¶ CSV â†’ Parquet ----------
def convert_one(csv_path: pathlib.Path):
    temp_path = None
    try:
        m = CSV_REGEX.search(csv_path.name)
        if not m:
            print(f"Skip {csv_path}")
            return {
                "status": "skipped",
                "path": str(csv_path),
                "reason": "filename pattern mismatch",
            }

        symbol = m.group("symbol")
        kind_or_interval = m.group("kind_or_interval")
        date_str = m.group("date")
        date_fmt = datetime.strptime(date_str, "%Y-%m-%d").date()

        # æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œç¬¬äºŒä¸ªå­—æ®µç¡®å®šæ•°æ®ç±»å‹
        if any(
            kline_type in str(csv_path)
            for kline_type in [
                "klines",
                "indexPriceKlines",
                "markPriceKlines",
                "premiumIndexKlines",
            ]
        ):
            # Kçº¿æ•°æ®: kind_or_interval æ˜¯æ—¶é—´é—´éš”
            interval = kind_or_interval
            # ä»è·¯å¾„ä¸­æ¨æ–­æ•°æ®ç±»å‹
            for kline_type in [
                "klines",
                "indexPriceKlines",
                "markPriceKlines",
                "premiumIndexKlines",
            ]:
                if kline_type in str(csv_path):
                    kind = kline_type
                    break
            else:
                kind = "klines"  # é»˜è®¤
        else:
            # å…¶ä»–æ•°æ®: kind_or_interval æ˜¯æ•°æ®ç±»å‹
            kind = kind_or_interval
            interval = ""

        # ç›®æ ‡å­ç›®å½•
        dst = DST_DIR / kind
        if interval:
            dst = dst / f"interval={interval}"
        dst = dst / f"date={date_fmt}"
        dst.mkdir(parents=True, exist_ok=True)
        parquet_path = dst / f"symbol={symbol}.parquet"

        # å¦‚æœå·²å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
        if parquet_path.exists():
            try:
                # å°è¯•è¯»å–éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                pq.read_metadata(parquet_path)
                return {
                    "status": "skipped",
                    "path": str(csv_path),
                    "reason": "already exists and valid",
                }
            except Exception:
                # æ–‡ä»¶æŸåï¼Œåˆ é™¤é‡æ–°åˆ›å»º
                print(f"âš  Corrupted file detected, removing: {parquet_path}")
                parquet_path.unlink()

        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¡®ä¿åŸå­æ€§å†™å…¥
        temp_path = parquet_path.with_suffix(".tmp")

        # è¯»å– CSV â†’ Arrow Table
        table = pv.read_csv(
            csv_path, read_options=pv.ReadOptions(autogenerate_column_names=False)
        )

        # æ£€æŸ¥ç©ºæ–‡ä»¶
        if len(table) == 0:
            return {"status": "skipped", "path": str(csv_path), "reason": "empty file"}

        # æ·»åŠ åˆ†åŒºåˆ—
        table = table.append_column("symbol", pa.array([symbol] * len(table)))
        if interval:
            table = table.append_column("interval", pa.array([interval] * len(table)))
        table = table.append_column("date", pa.array([str(date_fmt)] * len(table)))

        # å†™ Parquetï¼Œä¿æŒæŒ‰æ—¶é—´æ’åº
        sorter = (
            table.column_names.index("open_time")
            if "open_time" in table.column_names
            else None
        )
        if sorter is not None:
            indices = pa.compute.sort_indices(
                table, sort_keys=[("open_time", "ascending")]
            )
            table = table.take(indices)

        pq.write_table(table, temp_path, compression="zstd")

        # åŸå­æ€§é‡å‘½å
        temp_path.rename(parquet_path)
        return {"status": "success", "path": str(csv_path), "output": str(parquet_path)}

    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_path and temp_path.exists():
            temp_path.unlink()
        print(f"âœ— Error processing {csv_path}: {str(e)}")
        return {"status": "error", "path": str(csv_path), "error": str(e)}


# ---------- 2. å¤šè¿›ç¨‹éå† ----------
def batch_convert():
    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    DST_DIR.mkdir(parents=True, exist_ok=True)

    csv_files = list(SRC_DIR.rglob("*.csv"))
    total_files = len(csv_files)

    logging.info(f"å¼€å§‹å¤„ç† {total_files} ä¸ªCSVæ–‡ä»¶")
    logging.info(f"æºç›®å½•: {SRC_DIR}")
    logging.info(f"ç›®æ ‡ç›®å½•: {DST_DIR}")
    logging.info(f"ä½¿ç”¨CPUæ ¸å¿ƒæ•°: {CPU}")

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    with Pool(processes=CPU) as p:
        results = list(
            tqdm(
                p.imap(convert_one, csv_files),
                total=total_files,
                desc="Converting CSVâ†’Parquet",
                unit="files",
                dynamic_ncols=True,
            )
        )

    # ç»Ÿè®¡ç»“æœ
    stats = {"success": 0, "skipped": 0, "error": 0}
    errors = []
    successful_files = []

    for result in results:
        if result:
            stats[result["status"]] += 1
            if result["status"] == "error":
                errors.append(result)
                logging.error(f"Failed: {result['path']} - {result['error']}")
            elif result["status"] == "success":
                successful_files.append(result)
                # logging.info(f"Success: {result['path']} â†’ {result['output']}")

    # è¾“å‡ºè¯¦ç»†çš„ç»“æœæŠ¥å‘Š
    print(f"\n{'='*50}")
    print(f"ğŸ¯ CONVERSION SUMMARY")
    print(f"{'='*50}")
    print(f"ğŸ“ Total files processed: {total_files}")
    print(f"âœ… Successful conversions: {stats['success']}")
    print(f"â­ï¸  Skipped (existing/empty): {stats['skipped']}")
    print(f"âŒ Failed conversions: {stats['error']}")
    print(f"ğŸ“Š Success rate: {stats['success']/total_files*100:.1f}%")

    if errors:
        print(f"\n{'='*50}")
        print(f"âŒ ERROR DETAILS")
        print(f"{'='*50}")

        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
        error_types = {}
        for err in errors:
            error_msg = err["error"]
            error_type = error_msg.split(":")[0] if ":" in error_msg else "Unknown"
            if error_type not in error_types:
                error_types[error_type] = []
            error_types[error_type].append(err)

        for error_type, errs in error_types.items():
            print(f"\nğŸ” {error_type} ({len(errs)} files):")
            for err in errs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  â€¢ {pathlib.Path(err['path']).name}")
            if len(errs) > 5:
                print(f"  ... and {len(errs) - 5} more files")

        # å°†å®Œæ•´é”™è¯¯åˆ—è¡¨å†™å…¥æ–‡ä»¶
        error_log = LOG_PATH / "conversion_errors.log"
        with open(error_log, "w", encoding="utf-8") as f:
            f.write(f"Conversion Errors - {datetime.now()}\n")
            f.write("=" * 80 + "\n\n")
            for err in errors:
                f.write(f"FILE: {err['path']}\n")
                f.write(f"ERROR: {err['error']}\n")
                f.write("-" * 80 + "\n")

        print(f"\nğŸ“ Complete error log: {error_log}")
        logging.info(f"Saved detailed error log to {error_log}")

    # ç”ŸæˆæˆåŠŸæ–‡ä»¶æ¸…å•
    if successful_files:
        success_log = LOG_PATH / "conversion_success.log"
        with open(success_log, "w", encoding="utf-8") as f:
            f.write(f"Successful Conversions - {datetime.now()}\n")
            f.write("=" * 80 + "\n\n")
            for succ in successful_files:
                f.write(f"CSV: {succ['path']}\n")
                f.write(f"PARQUET: {succ['output']}\n")
                f.write("-" * 40 + "\n")

    logging.info(f"è½¬æ¢å®Œæˆ: {stats['success']}/{total_files} æˆåŠŸ")
    return stats["error"] == 0


# ---------- 2.5. æ¸…ç†æŸåçš„Parquetæ–‡ä»¶ ----------
def cleanup_corrupted():
    """æ‰«æå¹¶åˆ é™¤æŸåçš„parquetæ–‡ä»¶"""
    parquet_files = list(DST_DIR.rglob("*.parquet"))
    corrupted = []

    logging.info(f"å¼€å§‹æ£€æŸ¥ {len(parquet_files)} ä¸ªparquetæ–‡ä»¶çš„å®Œæ•´æ€§")

    # ä½¿ç”¨è¿›åº¦æ¡æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
    for pf in tqdm(parquet_files, desc="Checking parquet integrity", unit="files"):
        try:
            pq.read_metadata(pf)
        except Exception:
            logging.warning(f"å‘ç°æŸåæ–‡ä»¶: {pf}")
            corrupted.append(pf)

    if corrupted:
        print(f"\nâš ï¸  å‘ç° {len(corrupted)} ä¸ªæŸåçš„æ–‡ä»¶:")
        for cf in corrupted[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"  â€¢ {cf.relative_to(DST_DIR)}")
        if len(corrupted) > 10:
            print(f"  ... è¿˜æœ‰ {len(corrupted) - 10} ä¸ªæŸåæ–‡ä»¶")

        response = input(f"\nåˆ é™¤è¿™ {len(corrupted)} ä¸ªæŸåæ–‡ä»¶? [y/N]: ")
        if response.lower() == "y":
            for cf in tqdm(corrupted, desc="Deleting corrupted files", unit="files"):
                cf.unlink()
                logging.info(f"å·²åˆ é™¤æŸåæ–‡ä»¶: {cf}")
            print(f"âœ… å·²åˆ é™¤ {len(corrupted)} ä¸ªæŸåæ–‡ä»¶")
        else:
            print("âŒ å–æ¶ˆæ¸…ç†æ“ä½œ")
    else:
        print("âœ… æ‰€æœ‰parquetæ–‡ä»¶éƒ½å®Œæ•´æœ‰æ•ˆ")
        logging.info("æ‰€æœ‰parquetæ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")


# ---------- 3. åˆå§‹åŒ– DuckDB ----------
def init_duck_views():
    con = duckdb.connect(DUCK_DB)
    con.execute("INSTALL httpfs; LOAD httpfs;")  # å¦‚æœä½ è¦ S3ï¼Œå¯ç”¨
    con.execute("SET enable_progress_bar=true;")

    # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
    con.execute("SET memory_limit='8GB';")  # æ ¹æ®ä½ çš„å†…å­˜è°ƒæ•´
    con.execute("SET threads=8;")  # æ ¹æ®ä½ çš„CPUæ ¸å¿ƒæ•°è°ƒæ•´
    con.execute("SET enable_object_cache=true;")
    con.execute("SET preserve_insertion_order=false;")

    views = {
        "book_depth": DATA_PATH + "/bookDepth/*/*.parquet",
        "klines": DATA_PATH + "/klines/*/*/*.parquet",
        "index_price_klines": DATA_PATH + "/indexPriceKlines/*/*/*.parquet",
        "mark_price_klines": DATA_PATH + "/markPriceKlines/*/*/*.parquet",
        "premium_index_klines": DATA_PATH + "/premiumIndexKlines/*/*/*.parquet",
        "metrics": DATA_PATH + "/metrics/*/*.parquet",
    }

    for v, path in views.items():
        con.execute(
            f"CREATE OR REPLACE VIEW {v} AS SELECT * FROM parquet_scan('{path}');"
        )
    con.close()


# ---------- 4. ä¸­é—´ä»¶æŸ¥è¯¢ API ----------
class BinanceDuck:
    def __init__(self, db_path=DUCK_DB):
        self.con = duckdb.connect(db_path, read_only=True)
        # ä¼˜åŒ–è¿æ¥è®¾ç½®
        self.con.execute("SET memory_limit='8GB';")
        self.con.execute("SET threads=8;")
        self.con.execute("SET enable_object_cache=true;")
        self.con.execute("SET preserve_insertion_order=false;")

    def scan_15m(self, start_ts, end_ts, table="klines"):
        # è®¡ç®—æ—¥æœŸèŒƒå›´ç”¨äºåˆ†åŒºè£å‰ª
        from datetime import datetime

        start_date = datetime.fromtimestamp(start_ts / 1000).date()
        end_date = datetime.fromtimestamp(end_ts / 1000).date()

        # ç›´æ¥æŸ¥è¯¢parquetæ–‡ä»¶ï¼Œåˆ©ç”¨åˆ†åŒºè£å‰ª
        sql = f"""
        SELECT *
        FROM parquet_scan('{DATA_PATH}/{table}/interval=15m/date>={start_date}/*.parquet')
        WHERE open_time BETWEEN {start_ts} AND {end_ts}
        """
        return self.con.sql(sql).df()

    def scan_15m_optimized(self, start_ts, end_ts, symbols=None, table="klines"):
        """ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒç¬¦å·è¿‡æ»¤å’Œæ›´ç²¾ç¡®çš„åˆ†åŒºè£å‰ª"""
        from datetime import datetime, timedelta

        start_date = datetime.fromtimestamp(start_ts / 1000).date()
        end_date = datetime.fromtimestamp(end_ts / 1000).date()

        # ç”Ÿæˆæ—¥æœŸèŒƒå›´çš„parquetæ–‡ä»¶è·¯å¾„
        current_date = start_date
        file_patterns = []
        while current_date <= end_date:
            file_patterns.append(
                f"'{DATA_PATH}/{table}/interval=15m/date={current_date}/*.parquet'"
            )
            current_date += timedelta(days=1)

        if not file_patterns:
            return self.con.sql("SELECT * FROM (VALUES (1,1)) t(a,b) WHERE FALSE").df()

        files_str = ", ".join(file_patterns)

        sql = f"""
        SELECT *
        FROM parquet_scan([{files_str}])
        WHERE open_time BETWEEN {start_ts} AND {end_ts}
        """

        if symbols:
            if isinstance(symbols, str):
                symbols = [symbols]
            symbols_str = "', '".join(symbols)
            sql += f" AND symbol IN ('{symbols_str}')"

        return self.con.sql(sql).df()

    def trace_symbol(self, symbol, since_ts, table="klines"):
        # ä¼˜åŒ–ï¼šå…ˆæŒ‰æ—¥æœŸè¿‡æ»¤å†æŒ‰ç¬¦å·è¿‡æ»¤
        from datetime import datetime

        since_date = datetime.fromtimestamp(since_ts / 1000).date()

        sql = f"""
        SELECT *
        FROM parquet_scan('{DATA_PATH}/{table}/*/*/date>={since_date}/*.parquet')
        WHERE symbol='{symbol}'
          AND open_time >= {since_ts}
        ORDER BY open_time
        """
        return self.con.sql(sql).df()

    def scan_aggregated(self, start_ts, end_ts, symbols=None, table="klines"):
        """èšåˆæŸ¥è¯¢ï¼šè®¡ç®—OHLCVç»Ÿè®¡ä¿¡æ¯"""
        from datetime import datetime, timedelta

        start_date = datetime.fromtimestamp(start_ts / 1000).date()
        end_date = datetime.fromtimestamp(end_ts / 1000).date()

        current_date = start_date
        file_patterns = []
        while current_date <= end_date:
            file_patterns.append(
                f"'{DATA_PATH}/{table}/interval=15m/date={current_date}/*.parquet'"
            )
            current_date += timedelta(days=1)

        if not file_patterns:
            return self.con.sql("SELECT 1 WHERE FALSE").df()

        files_str = ", ".join(file_patterns)

        sql = f"""
        SELECT symbol,
               COUNT(*) as count,
               MIN(open_time) as first_time,
               MAX(open_time) as last_time,
               AVG(CAST(volume AS DOUBLE)) as avg_volume,
               MAX(CAST(high AS DOUBLE)) as max_price,
               MIN(CAST(low AS DOUBLE)) as min_price
        FROM parquet_scan([{files_str}])
        WHERE open_time BETWEEN {start_ts} AND {end_ts}
        """

        if symbols:
            if isinstance(symbols, str):
                symbols = [symbols]
            symbols_str = "', '".join(symbols)
            sql += f" AND symbol IN ('{symbols_str}')"

        sql += " GROUP BY symbol ORDER BY symbol"
        return self.con.sql(sql).df()

    def scan_single_symbol(self, symbol, start_ts, end_ts, table="klines"):
        """å•ä¸ªç¬¦å·æŸ¥è¯¢"""
        from datetime import datetime, timedelta

        start_date = datetime.fromtimestamp(start_ts / 1000).date()
        end_date = datetime.fromtimestamp(end_ts / 1000).date()

        current_date = start_date
        file_patterns = []
        while current_date <= end_date:
            file_patterns.append(
                f"'{DATA_PATH}/{table}/interval=15m/date={current_date}/symbol={symbol}.parquet'"
            )
            current_date += timedelta(days=1)

        if not file_patterns:
            return self.con.sql("SELECT 1 WHERE FALSE").df()

        files_str = ", ".join(file_patterns)

        sql = f"""
        SELECT *
        FROM parquet_scan([{files_str}])
        WHERE open_time BETWEEN {start_ts} AND {end_ts}
        ORDER BY open_time
        """
        return self.con.sql(sql).df()

    def scan_top_volume(self, start_ts, end_ts, limit=10, table="klines"):
        """æŸ¥è¯¢äº¤æ˜“é‡æœ€å¤§çš„ç¬¦å·"""
        from datetime import datetime, timedelta

        start_date = datetime.fromtimestamp(start_ts / 1000).date()
        end_date = datetime.fromtimestamp(end_ts / 1000).date()

        current_date = start_date
        file_patterns = []
        while current_date <= end_date:
            file_patterns.append(
                f"'{DATA_PATH}/{table}/interval=15m/date={current_date}/*.parquet'"
            )
            current_date += timedelta(days=1)

        if not file_patterns:
            return self.con.sql("SELECT 1 WHERE FALSE").df()

        files_str = ", ".join(file_patterns)

        sql = f"""
        SELECT symbol,
               SUM(CAST(volume AS DOUBLE)) as total_volume,
               COUNT(*) as record_count
        FROM parquet_scan([{files_str}])
        WHERE open_time BETWEEN {start_ts} AND {end_ts}
        GROUP BY symbol
        ORDER BY total_volume DESC
        LIMIT {limit}
        """
        return self.con.sql(sql).df()

    def close(self):
        self.con.close()


# ---------- 5. äº¤äº’å¼èœå• ----------
def show_menu():
    """æ˜¾ç¤ºä¸»èœå•"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ Binanceæ•°æ®ETLå·¥å…·")
    print(f"{'='*60}")
    print(f"ğŸ“Š æºç›®å½•: {SRC_DIR}")
    print(f"ğŸ¯ ç›®æ ‡ç›®å½•: {DST_DIR}")
    print(f"ğŸ“ æ—¥å¿—ç›®å½•: {LOG_PATH}")
    print(f"ğŸ—„ï¸  DuckDBæ•°æ®åº“: {DUCK_DB}")
    print(f"âš¡ CPUæ ¸å¿ƒæ•°: {CPU}")
    print(f"{'='*60}")
    print("è¯·é€‰æ‹©è¦æ‰§è¡Œçš„åŠŸèƒ½:")
    print()
    print("1. ğŸ“‚ ETLè½¬æ¢    - å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼")
    print("2. ğŸ—ƒï¸  åˆå§‹åŒ–DB   - åˆ›å»ºDuckDBè§†å›¾å’Œè¡¨ç»“æ„")
    print("3. ğŸ§¹ æ¸…ç†æŸå   - æ‰«æå¹¶åˆ é™¤æŸåçš„Parquetæ–‡ä»¶")
    print("4. ğŸ” æµ‹è¯•æŸ¥è¯¢   - è¿è¡Œæµ‹è¯•æŸ¥è¯¢éªŒè¯æ•°æ®")
    print("5. â„¹ï¸  æ˜¾ç¤ºçŠ¶æ€   - æ˜¾ç¤ºå½“å‰æ•°æ®çŠ¶æ€å’Œç»Ÿè®¡")
    print("6. âŒ é€€å‡º")
    print(f"{'='*60}")


def get_data_status():
    """è·å–å½“å‰æ•°æ®çŠ¶æ€"""
    csv_files = list(SRC_DIR.rglob("*.csv"))
    parquet_files = list(DST_DIR.rglob("*.parquet")) if DST_DIR.exists() else []

    print(f"\nğŸ“ˆ å½“å‰æ•°æ®çŠ¶æ€:")
    print(f"{'='*40}")
    print(f"ğŸ“„ CSVæ–‡ä»¶æ•°é‡: {len(csv_files):,}")
    print(f"ğŸ“¦ Parquetæ–‡ä»¶æ•°é‡: {len(parquet_files):,}")

    if csv_files:
        print(f"ğŸ“… CSVæ—¥æœŸèŒƒå›´: {get_date_range(csv_files)}")

    if parquet_files:
        print(f"ğŸ’¾ Parquetæ€»å¤§å°: {get_total_size(parquet_files)}")
        print(
            f"ğŸ“Š è½¬æ¢è¿›åº¦: {len(parquet_files)}/{len(csv_files)} ({len(parquet_files)/len(csv_files)*100:.1f}%)"
        )

    print(f"ğŸ—„ï¸  DuckDBå­˜åœ¨: {'âœ…' if pathlib.Path(DUCK_DB).exists() else 'âŒ'}")


def get_date_range(files):
    """è·å–æ–‡ä»¶æ—¥æœŸèŒƒå›´"""
    dates = []
    for f in files:
        m = CSV_REGEX.search(f.name)
        if m:
            try:
                date_obj = datetime.strptime(m.group("date"), "%Y-%m-%d").date()
                dates.append(date_obj)
            except:
                continue

    if dates:
        return f"{min(dates)} è‡³ {max(dates)}"
    return "æ— æ³•ç¡®å®š"


def get_total_size(files):
    """è·å–æ–‡ä»¶æ€»å¤§å°"""
    total_bytes = sum(f.stat().st_size for f in files if f.exists())

    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if total_bytes < 1024:
            return f"{total_bytes:.1f}{unit}"
        total_bytes /= 1024
    return f"{total_bytes:.1f}PB"


def run_performance_tests():
    """æ€§èƒ½æµ‹è¯•ï¼šæµ‹è¯•ä¸åŒæŸ¥è¯¢æ¨¡å¼"""
    import time

    print(f"\n{'='*60}")
    print(f"ğŸš€ æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    print(f"{'='*60}")

    try:
        db = BinanceDuck()

        # æµ‹è¯•åœºæ™¯å®šä¹‰
        test_scenarios = [
            {
                "name": "å…¨ç›˜çŸ­æ—¶é—´æ‰«æ",
                "desc": f"æ‰«ææ‰€æœ‰ç¬¦å·çš„{(TEST_SHORT_END - TEST_SHORT_START).seconds/3600:.1f}å°æ—¶æ•°æ®",
                "func": lambda: db.scan_15m_optimized(
                    TEST_SHORT_START_MS, TEST_SHORT_END_MS
                ),
                "type": "æ•°æ®å¯†é›†å‹",
            },
            {
                "name": "å•ç¬¦å·é•¿æ—¶åº",
                "desc": f"BTCUSDTçš„{(TEST_LONG_END - TEST_LONG_START).days}å¤©æ—¶é—´åºåˆ—",
                "func": lambda: db.scan_single_symbol(
                    "BTCUSDT", TEST_LONG_START_MS, TEST_LONG_END_MS
                ),
                "type": "æ—¶é—´è·¨åº¦å‹",
            },
            {
                "name": "çƒ­é—¨ç¬¦å·æ‰«æ",
                "desc": "çŸ­æ—¶é—´å†…å¤šä¸ªçƒ­é—¨ç¬¦å·",
                "func": lambda: db.scan_15m_optimized(
                    TEST_SHORT_START_MS,
                    TEST_SHORT_END_MS,
                    symbols=["BTCUSDT", "ETHUSDT", "BNBUSDT"],
                ),
                "type": "ç²¾å‡†è¿‡æ»¤å‹",
            },
            {
                "name": "äº¤æ˜“é‡ç»Ÿè®¡",
                "desc": "èšåˆåˆ†æäº¤æ˜“é‡æ’å",
                "func": lambda: db.scan_top_volume(
                    TEST_SHORT_START_MS, TEST_SHORT_END_MS, limit=20
                ),
                "type": "èšåˆåˆ†æå‹",
            },
        ]

        print(f"çŸ­æ—¶é—´èŒƒå›´: {TEST_SHORT_START} ~ {TEST_SHORT_END}")
        print(f"é•¿æ—¶é—´èŒƒå›´: {TEST_LONG_START} ~ {TEST_LONG_END}")
        print(
            f"\n{'åœºæ™¯åç§°':<15} {'ç±»å‹':<12} {'è€—æ—¶':<8} {'è®°å½•æ•°':<10} {'å¤„ç†é€Ÿåº¦':<12}"
        )
        print("-" * 60)

        for scenario in test_scenarios:
            query_start = time.time()
            try:
                result = scenario["func"]()
                query_time = time.time() - query_start
                count = len(result)
                rps = count / query_time if query_time > 0 else 0

                print(
                    f"{scenario['name']:<15} {scenario['type']:<12} {query_time:.2f}s {count:<10,} {rps:>8,.0f}/s"
                )

            except Exception as e:
                query_time = time.time() - query_start
                print(
                    f"{scenario['name']:<15} {scenario['type']:<12} {query_time:.2f}s {'å¤±è´¥':<10} {str(e)[:20]}"
                )

        db.close()

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def interactive_menu():
    """äº¤äº’å¼èœå•ä¸»å¾ªç¯"""
    while True:
        show_menu()
        try:
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-6): ").strip()

            if choice == "1":
                print(f"\nğŸš€ å¼€å§‹ETLè½¬æ¢...")
                success = batch_convert()
                input(f"\nè½¬æ¢{'å®Œæˆ' if success else 'å®Œæˆ(æœ‰é”™è¯¯)'}ï¼ŒæŒ‰Enterç»§ç»­...")

            elif choice == "2":
                print(f"\nğŸ—ƒï¸ åˆå§‹åŒ–DuckDB...")
                init_duck_views()
                print("âœ… DuckDBåˆå§‹åŒ–å®Œæˆ")
                input("æŒ‰Enterç»§ç»­...")

            elif choice == "3":
                print(f"\nğŸ§¹ å¼€å§‹æ¸…ç†æŸåæ–‡ä»¶...")
                cleanup_corrupted()
                input("æŒ‰Enterç»§ç»­...")

            elif choice == "4":
                print(f"\nğŸ” è¿è¡Œæ€§èƒ½æµ‹è¯•...")
                run_performance_tests()
                input("æŒ‰Enterç»§ç»­...")

            elif choice == "5":
                get_data_status()
                input("æŒ‰Enterç»§ç»­...")

            elif choice == "6":
                print(f"\nğŸ‘‹ å†è§!")
                break

            else:
                print(f"âŒ æ— æ•ˆé€‰é¡¹ '{choice}'ï¼Œè¯·è¾“å…¥ 1-6")
                input("æŒ‰Enterç»§ç»­...")

        except KeyboardInterrupt:
            print(f"\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆï¼Œå†è§!")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            input("æŒ‰Enterç»§ç»­...")


# ---------- 6. CLI ----------
if __name__ == "__main__":
    import sys

    # å¦‚æœæœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œç›´æ¥æ‰§è¡Œå¯¹åº”å‘½ä»¤
    if len(sys.argv) > 1:
        cmd = sys.argv[1].lower()

        if cmd == "etl":
            print("ğŸš€ æ‰§è¡ŒETLè½¬æ¢...")
            success = batch_convert()
            sys.exit(0 if success else 1)

        elif cmd == "init":
            print("ğŸ—ƒï¸ åˆå§‹åŒ–DuckDB...")
            init_duck_views()
            print("âœ… DuckDBåˆå§‹åŒ–å®Œæˆ")

        elif cmd == "cleanup":
            print("ğŸ§¹ æ¸…ç†æŸåæ–‡ä»¶...")
            cleanup_corrupted()

        elif cmd == "test":
            print("ğŸ” è¿è¡Œæµ‹è¯•æŸ¥è¯¢...")
            try:
                db = BinanceDuck()
                result = db.scan_15m(1690848000000, 1690855200000)
                print(f"âœ… æµ‹è¯•æŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(result)} æ¡è®°å½•")
                print(result.head())
                db.close()
            except Exception as e:
                print(f"âŒ æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}")
                sys.exit(1)

        elif cmd == "status":
            get_data_status()

        elif cmd in ["help", "-h", "--help"]:
            print("\nğŸš€ Binanceæ•°æ®ETLå·¥å…·")
            print("=" * 50)
            print("ç”¨æ³•:")
            print("  python csv2duckdb.py                # äº¤äº’å¼èœå•")
            print("  python csv2duckdb.py etl            # ETLè½¬æ¢")
            print("  python csv2duckdb.py init           # åˆå§‹åŒ–DuckDB")
            print("  python csv2duckdb.py cleanup        # æ¸…ç†æŸåæ–‡ä»¶")
            print("  python csv2duckdb.py test           # æµ‹è¯•æŸ¥è¯¢")
            print("  python csv2duckdb.py status         # æ˜¾ç¤ºçŠ¶æ€")
            print("  python csv2duckdb.py help           # æ˜¾ç¤ºå¸®åŠ©")

        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: '{cmd}'")
            print("å¯ç”¨å‘½ä»¤: etl, init, cleanup, test, status, help")
            print("æˆ–è€…è¿è¡Œ 'python csv2duckdb.py' è¿›å…¥äº¤äº’æ¨¡å¼")
            sys.exit(1)
    else:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯åŠ¨äº¤äº’å¼èœå•
        interactive_menu()
